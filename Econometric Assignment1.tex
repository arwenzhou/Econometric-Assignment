\documentclass[12pt]{article}
 \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\DeclareMathOperator*{\argmin}{argmin}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
 
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\newenvironment{answer}{{\noindent\it Answer.}}{\hfill \par}

 
\begin{document}

 
\title{Econometric Assignment 1}
\author{Xiang ZHOU 1155118711}
\maketitle
 
\begin{question}{2.1}
Find $\mathbb{E}(\mathbb{E}(\mathbb{E}(y|x_1,x_2,x_3)|x_1,x_2)|x_1)$.
\end{question}

\begin{proof}
$\mathbb{E}(y|x_1)$
\end{proof}

\begin{question}{2.2}
If $\mathbb{E}(y|x) = a+bx$, find $\mathbb{E}(yx)$ as a function of moments of $x$. 
\end{question}

\begin{proof}
Consider continuous condition.\\
For $\mathbb{E}(y|x) = a+bx = \int_{-\infty}^{\infty}f(y|x)dy$, then
\begin{align*}
E(yx) &= \int yxf(y,x)dydx  \\
&=\int yxf(y|x)f(x)dydx\\
&=\int xf(x)dx\int yf(y|x)dy\\
&=\int (ax+bx^2) f(x)dx\\
&=\int (b(x+\frac{a}{2b})^2-\frac{a^2}{4b})f(x)dx\\
&=b\int (x+\frac{a}{2b})^2f(x)dx-\int\frac{a^2}{4b}f(x)dx
\end{align*}
is the function of moments of $x$.
\end{proof}

\begin{question}{2.3}
Prove following statement using the law of iterated expectations.\\
\emph{For any function $h(x)$ s.t. $\mathbb{E}|h(x)e|<\infty$ then $\mathbb{E}(h(x)e) = 0$}.
\end{question}

\begin{proof}
The linear projection error $e = y-m(\mathbf{x}) = y - \mathbb{E}(y|x)$, then for $\mathbb{E}(h(x)e)<\infty$
\begin{align*}
\mathbb{E}(h(x)e) &= \mathbb{E}(h(x)y)-\mathbb{E}(h(x)\mathbb{E}(y|x))\\
&= \mathbb{E}(h(x)y)-\mathbb{E}(\mathbb{E}(h(x)y|x))\\
&= \mathbb{E}(h(x)y)-\mathbb{E}(h(x)y)\\
&= 0
\end{align*}
\end{proof}

\begin{question}{2.9}
Suppose you have two regressors: $x_1$ is binary (take values 0 and 1) and $x_2$ is categorical with 3 categories $(A,B,C)$. Write $\mathbb{E}(y|x_1,x_2)$ as a linear regression.
\end{question}

\begin{proof}
Transform $x_2$ to dummy variable $x_2, x_3$, then the new regression in linear form is 
$$\mathbb{E}(y|x_1,x_2,x_3) = \beta_1 x_1+ \beta_2 x_2+ \beta_3 x_3+ \beta_4$$
when
\begin{equation}
x_2 = \left\{
\begin{array}{lr}
1, & \emph{if A}, \\
0, & \emph{not A}.\\              
\end{array}
\right.
\end{equation}
and
\begin{equation}
x_3 = \left\{
\begin{array}{lr}
1, & \emph{if B}, \\
0, & \emph{not B}.\\              
\end{array}
\right.
\end{equation}
\end{proof}

\begin{question}{2.10}
True or False. If $y = x\beta +e, x\in \mathbb{R}$, and $\mathbb{E}(e|x) = 0$, then $\mathbb{E}(x^2e) = 0$.
\end{question}
\begin{answer}
T.
\end{answer}

\begin{question}{2.11}
True or False. If $y = x\beta +e, x\in \mathbb{R}$, and $\mathbb{E}(xe) = 0$, then $\mathbb{E}(x^2e) = 0$.
\end{question}
\begin{answer}
T.
\end{answer}

\begin{question}{2.12}
True or False. If $y = \mathbf{x}'\mathbf{\beta} +e$ and $\mathbb{E}(e|\mathbf{x}) = 0$, then $e$ is independent of $x$.
\end{question}
\begin{answer}
F. 2-order moment are not independent.
\end{answer}

\begin{question}{2.13}
True or False. If $y = \mathbf{x}'\mathbf{\beta} +e$ and $\mathbb{E}(\mathbf{x}e) = \mathbf{0}$, then $\mathbb{E}(e|\mathbf{x}) = 0$.
\end{question}
\begin{answer}
F.
\end{answer}

\begin{question}{2.14}
True or False. If $y = \mathbf{x}'\mathbf{\beta} +e$ and $\mathbb{E}(e^2|\mathbf{x}) = \sigma^2$, a constant, then $e$ is independent of $\mathbf{x}$.
\end{question}
\begin{answer}
F.
\end{answer}

\begin{question}{2.15}
Consider the intercept-only model $y = \alpha+e$ define as the best linear predictor. Show that $\alpha = \mathbb{E}(y)$.
\end{question}

\begin{proof}
For intercept-only model $y = \alpha+e$, in which $m(\mathbf{x}) = \mathbb{E}(y) = \alpha$, then
\begin{align*}
\mathbb{E}(y) &= \mathbb{E}(\alpha+e)\\
&= \mathbb{E}(\alpha)+\mathbb{E}(e)\\
&= \alpha
\end{align*}

\end{proof}

\begin{question}{2.18}
Suppose that 
$$x = \begin{pmatrix} 1\\x_2\\x_3 \end{pmatrix}$$
and $x_3 = \alpha_1+\alpha_2 x_2$ is a linear function of $x_2$.\\
(a) Show that $\mathcal{Q}_{x} = \mathbb{E}(xx')$ is not invertible.\\
(b) Use a linear transformation of $x$ to find an expression for the best linear predictor of $y$ given $x$. (Be explicit, do no just use the generalized inverse formula.)
\end{question}

\begin{proof}
(a) $\mathbf{x}^T = (1,x_2,\alpha_1+\alpha_2x_2)$, then we have
$$\mathbb{E}(\mathbf{xx}^T) = \mathbb{E}(A)$$
\begin{align*}
A &= \begin{pmatrix} 1&x_2& x_3\\
x_1&x_2^2&x_2x_3\\
x_2&x_2x_3&x_3^2
\end{pmatrix}\\
&=\begin{pmatrix} 1&x_2& \alpha_1+\alpha_2x_2\\
x_2&x_2^2&x_2(\alpha_2+\alpha_2x_2)\\
\alpha_1+\alpha_2x_2&x_2(\alpha_1+\alpha_2x_2)&(\alpha_1+\alpha_2x_2)^2
\end{pmatrix}
\end{align*}

and
\begin{multline}
    det(A) = x_1^2(\alpha_1+\alpha_2x_1)^2\cdot 1+x_1\cdot x_1(\alpha_1+\alpha_2x_1)\cdot x_1 +(\alpha_1+\alpha_2x_1)\cdot x_1 \cdot x_1(\alpha_1+\alpha_2x_1)- \\ (\alpha_1+\alpha_2x_1)\cdot(x_1^2)\cdot(\alpha_1+\alpha_2x_1) - x_1(\alpha_1+\alpha_2x_1) \cdot x_1(\alpha_1+\alpha_2x_1)\cdot 1-(\alpha_1+\alpha_2x_1)^2 \cdot x_1 \cdot x_1 = 0
\end{multline}
then $A$ is not full rank, and $\mathbb{E}(A)$ is not full rank too, in other words, $\mathbb{E}(\mathbf{xx}^T)$ is not invertible.\\


(b)We take a transformation of $B = \begin{pmatrix}
1-\alpha_1&-\alpha_2&1\\
-\alpha_1&1-\alpha_2&1\\
\end{pmatrix}$, then $B\mathbf{x} = \mathbf{x^*} = \begin{pmatrix}
1\\
x_2\\
\end{pmatrix}$.\\
Then if we have finite second order moment of $\mathbf{y}$ and $\mathbf{x}^*$, and $x_2$ is not a constant, in other words,
$\mathbb{E}(\mathbf{x}^*\mathbf{x}^{*T})$ is p.d.\\
Then the best linear predictor $\mathcal{P}(\mathbf{y}|\mathbf{x}^*) = \mathbf{x}^{*T}\boldsymbol{\beta}$ need to be found, where $\boldsymbol{\beta} = \argmin\limits_{\mathbf{b}\in \mathbb{R}}\mathbb{E}((\mathbf{y}-\mathbf{x}^{*T}\mathbf{b})^2) $.

$$S(\boldsymbol{\beta}) = \mathbb{E}^2(\mathbf{y}-\mathbf{x}^{*T}\boldsymbol{\beta}),$$with FOC
$$\frac{\partial}{\partial\boldsymbol{\beta}}S(\boldsymbol{\beta}) = -2\mathbb{E}(\mathbf{x^*y})+2\mathbb{E}(\mathbf{x^*x^*}^T)\boldsymbol{\beta} = \mathbf{0}$$
$$\boldsymbol{\beta} = \mathbb{E}^{-1}(\mathbf{x^*x^*}^T)\mathbb{E}(\mathbf{xy})$$
then
$$\mathcal{P}(\mathbf{y|x^*}) = \mathbf{x^*}^T\mathbb{E}(\mathbf{x^*x^*}^T)\mathbb{E}(\mathbf{x^*y})$$
where $\mathbf{x^*} = \begin{pmatrix}
1\\
x_2\\
\end{pmatrix}$.
\end{proof}

\begin{question}{2.21}
Consider the short and long projections
$$y = x\gamma_1+e$$
$$y = x\beta_1+x^2\beta_2+u$$
(a) Under what condition dose $\gamma_1 = \beta_1$?\\
(b) Now suppose the long projection is 
$$y = x\theta_1+x^3\theta_2+v$$
Is there a similar condition under which $\gamma_1 = \theta_1$?
\end{question}

\begin{proof}
(a) In long projection, $y = x\beta_1+x^2\beta_2+u$, then in short projection we have the solution of $\gamma_1$,
\begin{align*}
\gamma_1 &= \mathbb{E}(x^2)^{-1}\mathbb{E}(x y)\\
&=\mathbb{E}(x^2)^{-1}\mathbb{E}(x(x\beta_1+x^2\beta_2+u))\\
&=\beta_1+\mathbb{E}(x^2)^{-1}\mathbb{E}(x^3)\beta_2.
\end{align*}
Then if the $\mathbb{E}(x^2)^{-1}\mathbb{E}(x^3) = 0$ and $\mathbb{E}(x^2)\ne 0$ or $\beta_2 = 0$, $\gamma_1 = \beta_1$.\\
(b) Similar as (a), $\gamma_1  =\theta_1+\mathbb{E}(x^2)^{-1}\mathbb{E}(x^4)\theta_2$, then if the $\mathbb{E}(x^2)^{-1}\mathbb{E}(x^4) = 0$ and $\mathbb{E}(x^2)\ne 0$ or $\theta_2 = 0$, $\gamma_1 = \theta_1$.\\
But $\mathbb{E}(x^4) = Var(x^2)+\mathbb{E}^2(x^2)$, where $Var(x^2)\ge 0$ and $\mathbb{E}(x^2)>0$ with $\mathbb{E}(x^2)\ne 0$, then $\mathbb{E}(x^4)>0\ne0$.
Thus, the condition is $\theta_2 = 0$.
\end{proof}

\end{document}